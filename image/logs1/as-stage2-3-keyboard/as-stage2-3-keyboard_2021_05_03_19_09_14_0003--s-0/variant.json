{
  "algorithm_args": {
    "batch_size": 256,
    "collect_new_paths": true,
    "max_path_length": 200,
    "min_num_steps_before_training": 10,
    "num_epochs": 1000,
    "num_eval_steps_per_epoch": 2000,
    "num_expl_steps_per_train_loop": 1000,
    "num_train_loops_per_epoch": 10,
    "num_trains_per_train_loop": 10,
    "pure_expl_num_steps": 200000
  },
  "env_config": {
    "action_type": "disc_traj",
    "adapts": [
      "goal",
      "oracle",
      "reward"
    ],
    "env_kwargs": {
      "debug": false,
      "frame_skip": 5,
      "success_dist": 0.03
    },
    "env_name": "AnySwitch",
    "eval_gaze_path": "AnySwitch_gaze_data_eval.h5",
    "factories": [],
    "gaze_dim": 128,
    "gaze_path": "AnySwitch_gaze_data_train.h5",
    "oracle": "model",
    "oracle_kwargs": {
      "threshold": 0.5
    },
    "reward_max": 0,
    "reward_min": -1,
    "reward_type": "sparse",
    "seedid": 2000,
    "smooth_alpha": 0.8,
    "state_type": 0,
    "step_limit": 200
  },
  "expl_kwargs": {
    "eps": 0.1,
    "logit_scale": 10
  },
  "freeze_decoder": false,
  "freeze_rf": true,
  "from_pretrain": true,
  "layer_norm": true,
  "layer_size": 256,
  "logvar": -10,
  "pretrain_path": "AnySwitch_params_s1.pkl",
  "qf_lr": 0.0005,
  "replay_buffer_size": 2000000,
  "seedid": 2000,
  "trainer_kwargs": {
    "add_ood_term": -1,
    "discount": 0.995,
    "learning_rate": 0.0005,
    "min_q_weight": 0,
    "qf_criterion": null,
    "soft_target_tau": 0.001,
    "target_update_period": 1,
    "temp": 1,
    "use_noise": false,
    "use_supervised": "target_session"
  }
}