{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "import time\n",
    "import types\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.classic_control import rendering\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from baselines.deepq.simple import ActWrapper\n",
    "\n",
    "from pyglet.window import key as pygkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rc('savefig', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', 'cursor-control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tf_vars(scope, path):\n",
    "  saver = tf.train.Saver([v for v in tf.global_variables() if v.name.startswith(scope + '/')])\n",
    "  saver.save(sess, save_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tf_vars(scope, path):\n",
    "  saver = tf.train.Saver([v for v in tf.global_variables() if v.name.startswith(scope + '/')])\n",
    "  saver.restore(sess, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "    input_placeholder,\n",
    "    output_size,\n",
    "    scope,\n",
    "    n_layers=1,\n",
    "    size=256,\n",
    "    activation=tf.nn.relu,\n",
    "    output_activation=tf.nn.softmax,\n",
    "    reuse=False\n",
    "  ):\n",
    "  out = input_placeholder\n",
    "  with tf.variable_scope(scope, reuse=reuse):\n",
    "    for _ in range(n_layers):\n",
    "      out = tf.layers.dense(out, size, activation=activation)\n",
    "    out = tf.layers.dense(out, output_size, activation=output_activation)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(\n",
    "  rollouts, goal, title='', file_name=None):\n",
    "  plt.title(title)\n",
    "\n",
    "  for rollout in rollouts:\n",
    "    trajectory = [x[0] for x in rollout]\n",
    "    x, y = list(zip(*trajectory))[:2]\n",
    "    if is_succ(rollout):\n",
    "      cmap = mpl.cm.YlGn\n",
    "    else:\n",
    "      cmap = mpl.cm.gray\n",
    "    plt.scatter(x, y, c=range(len(x)), cmap=cmap, alpha=0.75, linewidth=0)\n",
    "    plt.scatter(\n",
    "      [goal[0]], [goal[1]], marker='*', color='yellow', \n",
    "      edgecolor='black', linewidth=1, s=300, alpha=0.5)\n",
    "    \n",
    "  plt.xlim([-0.05, 1.05])\n",
    "  plt.ylim([-0.05, 1.05])\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.axis('off')\n",
    "  if file_name is not None:\n",
    "    plt.savefig(os.path.join(data_dir, file_name), bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cart_to_polar = lambda v: np.array([\n",
    "  np.arctan2(v[0], v[1]), \n",
    "  #max(-max_speed, min(max_speed, np.linalg.norm(v)))\n",
    "  max_speed\n",
    "])\n",
    "\n",
    "normalize_polar = lambda v: np.array([\n",
    "  (-v[0] + 0.5*np.pi) % (2 * np.pi), \n",
    "  #max(-max_speed, min(max_speed, v[1]))\n",
    "  max_speed\n",
    "])\n",
    "\n",
    "def polar_to_cart(v):\n",
    "  return v[1]*np.array([np.cos(v[0]), np.sin(v[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perp_dist(pos, init_pos, goal):\n",
    "  u = pos - init_pos\n",
    "  v = goal - init_pos\n",
    "  w = init_pos + u.dot(v)\n",
    "  return np.linalg.norm(pos - w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_tasks = 10 # each task/env has a unique goal/target\n",
    "\n",
    "n_act_dim = 2 # vx, vy\n",
    "\n",
    "#grid_size = (28, 32) # ecog grid dimensions\n",
    "#bci_dim = grid_size[0]*grid_size[1] # number of BCI channels\n",
    "bci_dim = 384\n",
    "\n",
    "n_ext_obs_dim = 7 # number of external state observation dimensions\n",
    "n_obs_dim = n_ext_obs_dim + bci_dim\n",
    "\n",
    "gamma = 0.99 # discount factor\n",
    "max_ep_len = 500 # number of timesteps\n",
    "goal_dist_thresh = 0.05 # radius of goal\n",
    "succ_rew_bonus = 1 # for reaching goal\n",
    "max_speed = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_succ = lambda r: r[-1][-1]['succ']\n",
    "get_ttt = lambda r: r[-1][-1]['ttt']\n",
    "get_dfot = lambda r: r[-1][-1]['dfot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goals = np.random.random((n_tasks, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goals = []\n",
    "for ang in np.arange(0, 2*np.pi, 2*np.pi/8):\n",
    "  goals.append(polar_to_cart(np.array([ang, 0.5])) + 0.5)\n",
    "goals = np.array(goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(goals[:, 0], goals[:, 1], linewidth=0, color='gray', s=100, marker='*')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'goals.pkl'), 'wb') as f:\n",
    "  pickle.dump(goals, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'goals.pkl'), 'rb') as f:\n",
    "  goals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_2D_gaussian(data, **kwargs):\n",
    "  '''\n",
    "  Generate a 2D Gaussian.\n",
    "  data: N_X x N_Y x 2 matrix. N_X and N_Y are sizes of the ECoG grid.\n",
    "  kwargs: amp, cent_x, cent_y, spreadata_X, spreadata_Y\n",
    "  '''\n",
    "  amp = 2\n",
    "  cent_x, cent_y = 0, 0\n",
    "  spreadata_X, spreadata_Y = 7, 4.5\n",
    "  for key, value in kwargs.items():\n",
    "    if key is 'amp':\n",
    "      amp = value\n",
    "    elif key is 'spreadata_X':\n",
    "      spreadata_X = value\n",
    "    elif key is 'spreadata_Y':\n",
    "      spreadata_Y = value\n",
    "    elif key is 'cent_x':\n",
    "      cent_x = value\n",
    "    elif key is 'cent_y':\n",
    "      cent_y = value\n",
    "  F = amp*np.exp(\n",
    "    -(((data[:,:,0] - cent_x)**2)/(2*spreadata_X**2) +\n",
    "    ((data[:,:,1] - cent_y)**2)/(2*spreadata_Y**2)))\n",
    "  return F\n",
    "\n",
    "def features_2D(x, y, **kwargs):\n",
    "    ''' \n",
    "    Simulate 2D neural features for decoding direction.\n",
    "    Calls create_2D_gaussian() to generate features.\n",
    "    Arguments:\n",
    "    x = x direction magnitude\n",
    "    y = y direction magnitude\n",
    "    Keyword arguments:\n",
    "    noise = amplitude for uniformly distributed noise as % of amplitude (default 0)\n",
    "    grid_size = tuple for size of generated neural features (default 28 x 32)\n",
    "    plot = booelan for ploting the features (default False)\n",
    "    '''\n",
    "    # Check arguments\n",
    "    noise = 10\n",
    "    plot = False\n",
    "    for key, value in kwargs.items():\n",
    "      if key is 'noise':\n",
    "        noise = value\n",
    "      elif key is 'plot':\n",
    "        plot = value\n",
    "    x = int(grid_size[0]/2 + x)\n",
    "    x = np.max([np.min([x, grid_size[0]]), 0])\n",
    "    y = int(grid_size[1]/2 + y)\n",
    "    y = np.max([np.min([y, grid_size[1]]), 0])\n",
    "    data_X, data_Y = np.meshgrid(range(grid_size[0]), range(grid_size[1]))\n",
    "    data = np.stack((data_X, data_Y),axis=2)\n",
    "    kwargs.update({'cent_x':x, 'cent_y':y})\n",
    "    Z = create_2D_gaussian(data, **kwargs)\n",
    "    noise = noise/100 * np.max(Z)\n",
    "    Z += noise*np.random.rand(grid_size[1], grid_size[0])\n",
    "    if plot:\n",
    "      f = plt.figure()\n",
    "      plt.imshow(Z)\n",
    "      plt.colorbar()\n",
    "      plt.xlabel('X')\n",
    "      plt.ylabel('Y')\n",
    "      str_title = 'Simulated directional features'\n",
    "      str_title += '\\nCentroid = ({},{}) Noise amp = {:.3f}'\n",
    "      plt.title(str_title.format(x, y, noise))\n",
    "      plt.gca().invert_yaxis()\n",
    "      plt.show()\n",
    "    return Z.flatten()\n",
    "  \n",
    "mag = 1000\n",
    "internal_encode_obs = lambda action, ext_obs, goal: features_2D(*(polar_to_cart(action)*mag))\n",
    "def internal_decode_act(obs):\n",
    "  bci_obs = extract_bci_obs(obs)\n",
    "  Z = bci_obs.reshape((grid_size[1], grid_size[0]))\n",
    "  i, j = np.unravel_index(np.argmax(Z, axis=None), Z.shape)\n",
    "  i -= Z.shape[0] // 2\n",
    "  j -= Z.shape[1] // 2\n",
    "  return normalize_polar(cart_to_polar(np.array([j, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CursorControl(gym.Env):\n",
    "  metadata = {\n",
    "    'render.modes': ['human']\n",
    "  }\n",
    "  \n",
    "  def __init__(\n",
    "      self, \n",
    "      max_ep_len=max_ep_len, # max number of timesteps\n",
    "      goal=None, # target position\n",
    "      init_pos=None, # fixed initial position\n",
    "      rand_init=True, # True -> new, random initial position for each episode\n",
    "      return_to_init=False, # True -> need to return to initial position to succeed\n",
    "      human_user=False, # True -> human at the keyboard, instead of optimal synthetic user\n",
    "      blending=0, # between 0 and 1 (0 -> no blending, 1 -> ignore agent and take optimal actions)\n",
    "      rand_goal=False, # True -> new, random goal for each episode\n",
    "      using_reward_shaping=False,\n",
    "      vel_smoothing=0.8\n",
    "    ):\n",
    "    if not rand_init and init_pos is None:\n",
    "      raise ValueError\n",
    "      \n",
    "    self.observation_space = spaces.Box(np.zeros(n_obs_dim), np.ones(n_obs_dim))\n",
    "    self.action_space = spaces.Box(np.zeros(2), np.array([2*np.pi, max_speed]))\n",
    "    \n",
    "    self.max_ep_len = max_ep_len\n",
    "    self.goal = goal\n",
    "    self.return_to_init = return_to_init\n",
    "    self.init_pos = init_pos\n",
    "    self.return_to_init = return_to_init\n",
    "    self.rand_init = rand_init\n",
    "    self.blending = blending\n",
    "    self.rand_goal = rand_goal\n",
    "    self.human_user = human_user\n",
    "    self.using_reward_shaping = using_reward_shaping\n",
    "    self.vel_smoothing = vel_smoothing\n",
    "    \n",
    "    if not rand_goal:\n",
    "      assert goal is not None\n",
    "      self._set_goal(goal)\n",
    "    \n",
    "    self.pos = None # position\n",
    "    self.vel = None # velocity\n",
    "    self.curr_step = None # timestep in current episode\n",
    "    self.viewer = None\n",
    "    self.curr_obs = None # latest observation generated by self._obs()\n",
    "    self.succ = None # True -> most recent episode was successful\n",
    "    self.dfot = None\n",
    "    \n",
    "    self.goal_reached = None # True -> goal has been reached and agent needs to return to initial position\n",
    "    self.init_reached = None # True -> initial position has been reached after goal has been reached\n",
    "        \n",
    "  def _set_goal(self, goal):\n",
    "    self.goal = goal\n",
    "    self.optimal_user_policy = make_synth_user_policy(goal, using_ext_obs=True) # for blending\n",
    "    self.user_policy = make_human_user_policy() if self.human_user else make_synth_user_policy(\n",
    "      goal, using_ext_obs=True) # for BCI features\n",
    "    self.reward_func = make_reward_func(\n",
    "      goal, using_reward_shaping=self.using_reward_shaping, return_to_init=self.return_to_init)\n",
    "        \n",
    "  def _obs(self):\n",
    "    goal_reached_ind = np.array([1.0 if self.goal_reached else 0.0])\n",
    "    ext_obs = np.concatenate((self.pos, self.vel, self.init_pos, goal_reached_ind)) # external state observations (\"context\")\n",
    "    int_act = self.user_policy(ext_obs) # intended action\n",
    "    bci_obs = internal_encode_obs(int_act, ext_obs, self.goal) # BCI output\n",
    "    self.curr_obs = np.concatenate((ext_obs, bci_obs))\n",
    "    return self.curr_obs\n",
    "  \n",
    "  def _step(self, action):   \n",
    "    opt_act = self.optimal_user_policy(extract_ext_obs(self.curr_obs))\n",
    "    action = self.blending * opt_act + (1 - self.blending) * action\n",
    "    \n",
    "    action = polar_to_cart(normalize_polar(action))\n",
    "      \n",
    "    self.vel = self.vel_smoothing * self.vel + (1 - self.vel_smoothing) * action.ravel()\n",
    "    self.pos += self.vel\n",
    "    oob = (self.pos < 0).any() or (self.pos >= 1).any()\n",
    "    self.pos = np.minimum(np.ones(2), np.maximum(np.zeros(2), self.pos))\n",
    "    \n",
    "    self.dfot += perp_dist(self.pos, self.init_pos, self.goal)\n",
    "        \n",
    "    self.curr_step += 1\n",
    "    \n",
    "    if not self.goal_reached:\n",
    "      self.goal_reached = (np.abs(self.pos - self.goal) <= goal_dist_thresh).all()\n",
    "      \n",
    "    if self.return_to_init:\n",
    "      init_reached = (np.abs(self.pos - self.init_pos) <= goal_dist_thresh).all()\n",
    "      self.succ = self.goal_reached and init_reached\n",
    "    else:\n",
    "      self.succ = self.goal_reached\n",
    "      \n",
    "    oot = self.curr_step >= self.max_ep_len # out of time\n",
    "    \n",
    "    obs = self._obs()\n",
    "    r = self.reward_func(self.prev_obs, action, obs)\n",
    "    done = oot or self.succ or oob\n",
    "    info = {'goal': self.goal, 'succ': self.succ, 'ttt': self.curr_step, 'dfot': self.dfot}\n",
    "      \n",
    "    self.prev_obs = obs\n",
    "    \n",
    "    return obs, r, done, info\n",
    "    \n",
    "  def _reset(self):\n",
    "    if self.rand_init:\n",
    "      self.pos = np.random.random(2)\n",
    "      self.init_pos = copy(self.pos)\n",
    "    else:\n",
    "      self.pos = copy(self.init_pos)\n",
    "      \n",
    "    self.vel = np.zeros(2)\n",
    "    \n",
    "    if self.rand_goal:\n",
    "      self._set_goal(np.random.random(2))\n",
    "    \n",
    "    self.succ = False\n",
    "    self.goal_reached = False\n",
    "    self.init_reached = False\n",
    "    self.dfot = 0\n",
    "      \n",
    "    self.curr_step = 0\n",
    "    self.prev_obs = self._obs()\n",
    "    return self.prev_obs\n",
    "  \n",
    "  def _render(self, mode='human', close=False):\n",
    "    if close:\n",
    "      if self.viewer is not None:\n",
    "        self.viewer.close()\n",
    "        self.viewer = None\n",
    "      return\n",
    "    \n",
    "    if self.viewer is None:\n",
    "      self.viewer = rendering.SimpleImageViewer()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    canvas = FigureCanvas(fig)\n",
    "    \n",
    "    size = 100\n",
    "    swap_goal_colors_flag = not self.goal_reached or not self.return_to_init\n",
    "    \n",
    "    plt.scatter(\n",
    "      [self.goal[0]], [self.goal[1]], \n",
    "      color=('green' if swap_goal_colors_flag else 'gray'), \n",
    "      linewidth=0, alpha=0.75, marker='o', s=size*5\n",
    "    )\n",
    "    \n",
    "    if self.return_to_init:\n",
    "      plt.scatter(\n",
    "        [self.init_pos[0]], [self.init_pos[1]], \n",
    "        color=('gray' if swap_goal_colors_flag else 'green'), \n",
    "        linewidth=0, alpha=0.75, marker='s', s=size\n",
    "      )\n",
    "      \n",
    "    plt.scatter(\n",
    "      [self.pos[0]], [self.pos[1]], \n",
    "      color=('orange' if not self.succ else 'teal'), linewidth=0, \n",
    "      alpha=0.75, s=size\n",
    "    )\n",
    "    \n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    agg = canvas.switch_backends(FigureCanvas)\n",
    "    agg.draw()\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    self.viewer.imshow(\n",
    "      np.fromstring(agg.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_goal_reached_ind = lambda obs: obs[n_ext_obs_dim-1]\n",
    "extract_pos = lambda obs: obs[:2]\n",
    "extract_init_pos = lambda obs: obs[n_ext_obs_dim-3:n_ext_obs_dim-1]\n",
    "extract_bci_obs = lambda obs: obs[n_ext_obs_dim:]\n",
    "extract_ext_obs = lambda obs: obs[:n_ext_obs_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simulate user with optimal intended actions\n",
    "# that go directly to the goal, then directly back to the initial position if needed\n",
    "def make_synth_user_policy(goal, using_ext_obs=False):\n",
    "  def synth_user_policy(obs):\n",
    "    if not using_ext_obs:\n",
    "      obs = extract_ext_obs(obs)\n",
    "    init_pos = extract_init_pos(obs)\n",
    "    g = goal if extract_goal_reached_ind(obs) == 0 else init_pos\n",
    "    p = extract_pos(obs)\n",
    "    v = g - p\n",
    "    return cart_to_polar(v)\n",
    "  return synth_user_policy\n",
    "\n",
    "human_user = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instead of simulating ground-truth user actions, get keyboard input\n",
    "init_human_action = lambda: 4 # noop\n",
    "human_action = init_human_action()\n",
    "human_active = False\n",
    "\n",
    "UP = pygkey.LEFT\n",
    "DOWN = pygkey.RIGHT\n",
    "RIGHT = pygkey.UP\n",
    "LEFT = pygkey.DOWN\n",
    "\n",
    "def key_press(key, mod):\n",
    "  global human_action\n",
    "  global human_active\n",
    "  human_active = True\n",
    "  a = int(key)\n",
    "  if a == LEFT:\n",
    "    human_action = 0\n",
    "  elif a == RIGHT:\n",
    "    human_action = 1\n",
    "  elif a == UP:\n",
    "    human_action = 2\n",
    "  elif a == DOWN:\n",
    "    human_action = 3\n",
    "    \n",
    "def key_release(key, mod):\n",
    "  global human_action\n",
    "  a = int(key)\n",
    "  if a in [LEFT, RIGHT, UP, DOWN]:\n",
    "    human_action = 4\n",
    "    \n",
    "cont_act_of_disc = np.array([\n",
    "  [-max_speed, 0], # left\n",
    "  [max_speed, 0], # right\n",
    "  [0, max_speed], # up\n",
    "  [0, -max_speed], # down\n",
    "  [0, 0] # noop\n",
    "])\n",
    "  \n",
    "human_policy = lambda obs: cont_act_of_disc[human_action, :]\n",
    "make_human_user_policy = lambda *args, **kwargs: human_policy\n",
    "\n",
    "#human_user = True # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_reward_func(goal, using_reward_shaping=False, return_to_init=False):\n",
    "\n",
    "  def reward_shaping(obs, init_pos, goal_reached_ind):\n",
    "    # penalize distance to target\n",
    "    pos = extract_pos(obs)\n",
    "    if not return_to_init or (return_to_init and goal_reached_ind == 0):\n",
    "      phi = -np.linalg.norm(pos - goal)\n",
    "      if return_to_init and goal_reached_ind == 0:\n",
    "        phi += -np.linalg.norm(goal - init_pos)\n",
    "    else:\n",
    "      phi = -np.linalg.norm(pos - init_pos)\n",
    "    return phi\n",
    "\n",
    "  def reward_func(prev_obs, action, obs):\n",
    "    pos = extract_pos(obs)\n",
    "    init_pos = extract_init_pos(obs)\n",
    "    goal_reached_ind = extract_goal_reached_ind(obs)\n",
    "    \n",
    "    if ((np.abs(pos - goal) <= goal_dist_thresh).all() and not return_to_init) or (\n",
    "      (np.abs(pos - init_pos) <= goal_dist_thresh).all() and goal_reached_ind == 1 and return_to_init):\n",
    "      r = succ_rew_bonus # bonus for reaching target\n",
    "    else:\n",
    "      r = 0\n",
    "      \n",
    "    if using_reward_shaping:\n",
    "      prev_goal_reached_ind = extract_goal_reached_ind(prev_obs)\n",
    "      r += gamma * reward_shaping(obs, init_pos, goal_reached_ind) - reward_shaping(\n",
    "        prev_obs, init_pos, goal_reached_ind) # standard reward shaping formula\n",
    "\n",
    "    return r\n",
    "  \n",
    "  return reward_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one env/task per goal\n",
    "envs = [CursorControl(goal=goal, human_user=human_user) for goal in goals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_ep(policy, env, max_ep_len=max_ep_len, render=False, blending=0, human_user=False):\n",
    "  if human_user:\n",
    "    global human_action\n",
    "    human_action = init_human_action()\n",
    "    render = True\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    env.unwrapped.viewer.window.on_key_press = key_press\n",
    "    env.unwrapped.viewer.window.on_key_release = key_release\n",
    "    \n",
    "  old_blending = copy(env.blending)\n",
    "  env.blending = blending\n",
    "  \n",
    "  obs = env.reset()\n",
    "  \n",
    "  try:\n",
    "    policy.reset()\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  done = False\n",
    "  prev_obs = obs\n",
    "  rollout = []\n",
    "  \n",
    "  for step_idx in range(max_ep_len+1):\n",
    "    if done:\n",
    "      break\n",
    "      \n",
    "    try:\n",
    "      policy.observe(obs)\n",
    "      action = policy.act()\n",
    "    except:\n",
    "      action = policy(obs)\n",
    "      \n",
    "    obs, r, done, info = env.step(action)\n",
    "    \n",
    "    rollout.append((prev_obs, action, r, obs, float(done), info))\n",
    "    prev_obs = obs\n",
    "    if render:\n",
    "      env.render()\n",
    "      \n",
    "  env.blending = old_blending\n",
    "  \n",
    "  if render:\n",
    "    env.close()\n",
    "    \n",
    "  if human_user:\n",
    "    global human_active\n",
    "    human_active = False\n",
    "    \n",
    "  return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oracle_policies = [make_synth_user_policy(env.goal) for env in envs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oracle_decoder_policy = internal_decode_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed, random, linear decoder\n",
    "D_rand = np.random.random((n_act_dim, n_obs_dim))\n",
    "rand_decoder_policy = D_rand.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit bci simulator to recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_dir = os.path.join(data_dir, 'Bravo1_PythonData')\n",
    "\n",
    "screen_size = 250\n",
    "normalize_pos = lambda p: (p + screen_size) / (2 * screen_size)\n",
    "normalize_vel = lambda v: v / (2 * screen_size)\n",
    "\n",
    "def rollout_from_raw_rec(raw_rec):\n",
    "  goal = normalize_pos(raw_rec['TargetPosition'])\n",
    "  reward_func = make_reward_func(goal)\n",
    "\n",
    "  contexts = raw_rec['CursorState']\n",
    "  bci_outs = raw_rec['NeuralFeatures']\n",
    "  assert contexts.shape[1] == bci_outs.shape[1]\n",
    "  T_end = contexts.shape[1] - 1\n",
    "  \n",
    "  T_start = 0\n",
    "  while (contexts[:2, T_start] == contexts[:2, T_start+1]).all():\n",
    "    T_start += 1\n",
    "  T_start += 1\n",
    "  \n",
    "  init_pos = normalize_pos(contexts[:2, T_start])\n",
    "  goal_reached = False\n",
    "  goal_reached_ind = np.array([1.0 if goal_reached else 0.0])\n",
    "  action = None\n",
    "  succ = False\n",
    "  info = {'goal': goal, 'succ': succ}\n",
    "\n",
    "  rollout = []\n",
    "  for t in range(T_start, T_end):\n",
    "    pos = normalize_pos(contexts[:2, t])\n",
    "    vel = normalize_vel(contexts[2:4, t])\n",
    "    bci_feats = bci_outs[:, t]\n",
    "    obs = np.concatenate((pos, vel, init_pos, goal_reached_ind, bci_feats))\n",
    "\n",
    "    if t > T_start:\n",
    "      done = t == T_end - 1\n",
    "      r = reward_func(prev_obs, action, obs)\n",
    "      rollout.append((prev_obs, action, r, obs, float(done), info))\n",
    "\n",
    "    prev_obs = obs\n",
    "  return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_rollouts = []\n",
    "for sess_dir in os.listdir(rec_dir):\n",
    "  if os.path.isdir(os.path.join(rec_dir, sess_dir)):\n",
    "    for rec_file in os.listdir(os.path.join(rec_dir, sess_dir)):\n",
    "      if rec_file.endswith('.pkl'):\n",
    "        with open(os.path.join(rec_dir, sess_dir, rec_file), 'rb') as f:\n",
    "          raw_rec = pickle.load(f)\n",
    "          rollout = rollout_from_raw_rec(raw_rec)\n",
    "          rec_rollouts.append(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'rec_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(rec_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'rec_rollouts.pkl'), 'rb') as f:\n",
    "  rec_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_context = lambda ext_obs, goal: np.concatenate((ext_obs, goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_rollouts(rollouts):\n",
    "  contexts = []\n",
    "  bci_outs = []\n",
    "  obses = []\n",
    "  acts = []\n",
    "  for rollout in rollouts:\n",
    "    more_obses = list(zip(*rollout))[0]\n",
    "    goal = rollout[0][-1]['goal']\n",
    "    more_contexts = [make_context(extract_ext_obs(obs), goal) for obs in more_obses]\n",
    "    more_bci_outs = [extract_bci_obs(obs) for obs in more_obses]\n",
    "    more_acts = [cart_to_polar(goal - extract_pos(obs)) for obs in more_obses]\n",
    "    contexts.extend(more_contexts)\n",
    "    bci_outs.extend(more_bci_outs)\n",
    "    obses.extend(more_obses)\n",
    "    acts.extend(more_acts)\n",
    "  contexts = np.array(contexts)\n",
    "  bci_outs = np.array(bci_outs)\n",
    "  obses = np.array(obses)\n",
    "  acts = np.array(acts)\n",
    "  return contexts, bci_outs, obses, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contexts = None\n",
    "bci_outs = None\n",
    "train_idxes = None\n",
    "val_batch = None\n",
    "obses = None\n",
    "acts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_rec_rollouts(rec_rollouts):\n",
    "  global contexts\n",
    "  global bci_outs\n",
    "  global train_idxes\n",
    "  global val_batch\n",
    "  global obses\n",
    "  global acts\n",
    "  \n",
    "  vectorized_rec_rollouts = vectorize_rollouts(rec_rollouts)\n",
    "\n",
    "  contexts, bci_outs, obses, acts = vectorized_rec_rollouts\n",
    "  idxes = list(range(contexts.shape[0]))\n",
    "\n",
    "  random.shuffle(idxes)\n",
    "  n_train_examples = int(0.9 * len(idxes))\n",
    "  train_idxes = idxes[:n_train_examples]\n",
    "  val_idxes = idxes[n_train_examples:]\n",
    "  val_batch = contexts[val_idxes], bci_outs[val_idxes], obses[val_idxes], acts[val_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_rec_rollouts(rec_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contexts.shape, bci_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BEGIN DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = TSNE(n_components=2).fit_transform(bci_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(size):\n",
    "  idxes = random.sample(train_idxes, size)\n",
    "  batch = contexts[idxes], bci_outs[idxes], obses[idxes], acts[idxes]\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 100000\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "\n",
    "val_update_freq = 100\n",
    "\n",
    "n_layers = 1\n",
    "layer_size = 256\n",
    "activation = tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'bci_sim_scope.pkl'), 'rb') as f:\n",
    "  bci_sim_enc_scope, bci_sim_dec_scope = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bci_sim_enc_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bci_sim_dec_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goal_dim = 2\n",
    "context_ph = tf.placeholder(tf.float32, [None, n_ext_obs_dim + goal_dim]) # see make_context\n",
    "bci_out_ph = tf.placeholder(tf.float32, [None, bci_dim])\n",
    "\n",
    "bci_out = build_mlp(\n",
    "  context_ph, bci_dim, bci_sim_enc_scope, \n",
    "  n_layers=n_layers, size=layer_size,\n",
    "  activation=activation, output_activation=None\n",
    ")\n",
    "\n",
    "enc_loss = tf.reduce_mean((bci_out - bci_out_ph)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_ph = tf.placeholder(tf.float32, [None, n_obs_dim])\n",
    "act_ph = tf.placeholder(tf.float32, [None, n_act_dim])\n",
    "\n",
    "decoded_act = build_mlp(\n",
    "  obs_ph, n_act_dim, bci_sim_dec_scope, \n",
    "  n_layers=n_layers, size=layer_size,\n",
    "  activation=activation, output_activation=None\n",
    ")\n",
    "\n",
    "dec_loss = tf.reduce_mean((act_ph - decoded_act)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = enc_loss + dec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trained_bci_sim_enc(ext_obs, goal):\n",
    "  context = make_context(ext_obs, goal)\n",
    "  with tf.variable_scope(bci_sim_enc_scope, reuse=tf.AUTO_REUSE):\n",
    "    bci_feats = sess.run(bci_out, feed_dict={context_ph: context[None, :]})[0, :]\n",
    "  return bci_feats\n",
    "\n",
    "def trained_bci_sim_dec(obs):\n",
    "  with tf.variable_scope(bci_sim_dec_scope, reuse=tf.AUTO_REUSE):\n",
    "    act = sess.run(decoded_act, feed_dict={obs_ph: obs[None, :]})[0, :]\n",
    "  return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_batch_loss(batch, step=False, t=None):\n",
    "  batch_context, batch_bci_out, batch_obs, batch_act = batch \n",
    "  feed_dict = {\n",
    "    context_ph: batch_context,\n",
    "    bci_out_ph: batch_bci_out,\n",
    "    obs_ph: batch_obs,\n",
    "    act_ph: batch_act\n",
    "  }\n",
    "  [loss_eval, enc_loss_eval, dec_loss_eval] = sess.run([loss, enc_loss, dec_loss], feed_dict=feed_dict)\n",
    "  \n",
    "  if step:\n",
    "    sess.run(update_op, feed_dict=feed_dict)\n",
    "  \n",
    "  d = {'loss': loss_eval, 'enc_loss': enc_loss_eval, 'dec_loss': dec_loss_eval}\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_logs = {\n",
    "  'train_loss': [],\n",
    "  'val_loss': [],\n",
    "  'train_enc_loss': [],\n",
    "  'val_enc_loss': [],\n",
    "  'train_dec_loss': [],\n",
    "  'val_dec_loss': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_log = None\n",
    "while len(train_logs['train_loss']) < iterations:\n",
    "  batch = sample_batch(batch_size)\n",
    "  \n",
    "  t = len(train_logs['train_loss'])\n",
    "  train_log = compute_batch_loss(batch, step=True, t=t)\n",
    "  if val_log is None or t % val_update_freq == 0:\n",
    "    val_log = compute_batch_loss(val_batch, step=False, t=t)\n",
    "  \n",
    "  print('%d %d %f %f %f %f' % (\n",
    "    t, iterations, train_log['loss'], val_log['loss'], val_log['enc_loss'], val_log['dec_loss']))\n",
    "  \n",
    "  for k, v in train_log.items():\n",
    "    train_logs['%s%s' % ('train_' if 'loss' in k else '', k)].append(v)\n",
    "  for k, v in val_log.items():\n",
    "    train_logs['%s%s' % ('val_' if 'loss' in k else '', k)].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation Encoder Loss')\n",
    "plt.plot(train_logs['val_enc_loss'])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation Decoder Loss')\n",
    "plt.plot(train_logs['val_dec_loss'])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "internal_encode_obs = lambda action, ext_obs, goal: trained_bci_sim_enc(ext_obs, goal)\n",
    "internal_decode_act = trained_bci_sim_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'bci_sim_scope.pkl'), 'wb') as f:\n",
    "  pickle.dump((bci_sim_enc_scope, bci_sim_dec_scope), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tf_vars(bci_sim_enc_scope, os.path.join(data_dir, 'bci_sim_enc.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tf_vars(bci_sim_dec_scope, os.path.join(data_dir, 'bci_sim_dec.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tf_vars(bci_sim_enc_scope, os.path.join(data_dir, 'bci_sim_enc.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tf_vars(bci_sim_dec_scope, os.path.join(data_dir, 'bci_sim_dec.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oracle_decoder_policy = internal_decode_act\n",
    "\n",
    "# one env/task per goal\n",
    "envs = [CursorControl(goal=goal, human_user=human_user) for goal in goals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity-check env, decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollout = run_ep(oracle_policies[task_idx], envs[task_idx], render=True, human_user=human_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollout = run_ep(oracle_decoder_policy, envs[task_idx], render=True, human_user=human_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rollout = run_ep(rand_decoder_policy, envs[task_idx], render=True, human_user=human_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "envs[task_idx].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate oracle and random decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_eval_rollouts = 100\n",
    "\n",
    "def make_env(train_goal=True):\n",
    "  test_goal = np.random.random(2) if not train_goal else goals[np.random.choice(\n",
    "    list(range(n_tasks)))]\n",
    "  env = CursorControl(goal=test_goal)\n",
    "  return env\n",
    "\n",
    "def compute_act_pred_err(decoder_policy):\n",
    "  errs = []\n",
    "  for rollout in rec_rollouts:\n",
    "    goal = rollout[0][-1]['goal']\n",
    "    oracle_policy = make_synth_user_policy(goal)\n",
    "    try:\n",
    "      decoder_policy.reset()\n",
    "    except:\n",
    "      pass\n",
    "    for obs, act, rew, next_obs, done, info in rollout:\n",
    "      opt_act = oracle_policy(obs)\n",
    "      try:\n",
    "        decoder_policy.observe(obs)\n",
    "        decoded_act = decoder_policy.act()\n",
    "      except:\n",
    "        decoded_act = decoder_policy(obs)\n",
    "      errs.append(np.linalg.norm(opt_act - decoded_act))\n",
    "  return np.mean(errs)\n",
    "\n",
    "def evaluate_decoder_policy(decoder_policy, env=None, n_rollouts=n_eval_rollouts):\n",
    "  if env is None:\n",
    "    env = make_env(train_goal=False)\n",
    "  rollouts = [run_ep(\n",
    "    decoder_policy, env, render=human_user, \n",
    "    blending=0, human_user=human_user) for _ in range(n_rollouts)]\n",
    "  perf = {\n",
    "    'rew': np.mean([sum(x[2] for x in rollout) for rollout in rollouts]),\n",
    "    'succ': np.mean([1 if is_succ(rollout) else 0 for rollout in rollouts]),\n",
    "    'ttt': np.mean([get_ttt(rollout) for rollout in rollouts]),\n",
    "    'dfot': np.mean([get_dfot(rollout) for rollout in rollouts]),\n",
    "    'ape': compute_act_pred_err(decoder_policy)\n",
    "  }\n",
    "  return rollouts, perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oracle_rollouts, oracle_perf = evaluate_decoder_policy(oracle_decoder_policy, n_rollouts=n_eval_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'oracle_eval.pkl'), 'wb') as f:\n",
    "  pickle.dump((oracle_rollouts, oracle_perf), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'oracle_eval.pkl'), 'rb') as f:\n",
    "  oracle_rollouts, oracle_perf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_rollouts, rand_perf = evaluate_decoder_policy(rand_decoder_policy, n_rollouts=n_eval_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'rand_eval.pkl'), 'wb') as f:\n",
    "  pickle.dump((rand_rollouts, rand_perf), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'rand_eval.pkl'), 'rb') as f:\n",
    "  rand_rollouts, rand_perf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train decoder with imitation learning\n",
    "\n",
    "[Neuroprosthetic decoder training as imitation learning](https://arxiv.org/abs/1511.04156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_demo_rollouts_per_task = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_policies = [rand_decoder_policy for _ in range(n_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_blending = 0.75 # mixture coefficient for random vs. oracle decoder (1 -> pure oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_actions(rollout, policy):\n",
    "  for i, x in enumerate(rollout):\n",
    "    x = list(x)\n",
    "    x[-1]['action_taken'] = x[1]\n",
    "    x[1] = policy(x[0]) # replace taken action with action label\n",
    "    rollout[i] = tuple(x)\n",
    "  return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_rollouts = [label_actions(run_ep(\n",
    "  demo_policies[task_idx], env, render=human_user, \n",
    "  blending=demo_blending, human_user=human_user\n",
    "), oracle_policies[task_idx]) for _ in range(\n",
    "  n_demo_rollouts_per_task) for task_idx, env in enumerate(envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_rollouts += [label_actions(\n",
    "  rollout, make_synth_user_policy(rollout[0][-1]['goal'])) for rollout in rec_rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'demo_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(demo_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'demo_rollouts.pkl'), 'rb') as f:\n",
    "  demo_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max number of timesteps into the past \n",
    "# that the RNN decoder can look at\n",
    "history_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_mask(i, n): # for RNN training\n",
    "  x = np.zeros(n)\n",
    "  x[:i] = 1\n",
    "  return x\n",
    "\n",
    "pad_obses = lambda obses, n: list(obses) + [np.zeros(obses[-1].shape)] * (n - len(obses))\n",
    "pad_acts = lambda acts, n: list(acts) + [np.zeros(acts[-1].shape)] * (n - len(acts))\n",
    "\n",
    "def vectorize_rollouts(rollouts):\n",
    "  obses = []\n",
    "  actions = []\n",
    "  masks = []\n",
    "  for rollout in rollouts:\n",
    "    more_obses, more_actions = list(zip(*rollout))[:2]\n",
    "    for i in range(max(1, len(more_obses)-history_len+1)):\n",
    "      unpadded_obses = more_obses[i:i+history_len]\n",
    "      obses.append(pad_obses(unpadded_obses, history_len))\n",
    "      actions.append(pad_acts(more_actions[i:i+history_len], history_len))\n",
    "      masks.append(build_mask(len(unpadded_obses), history_len))\n",
    "  obses = np.array(obses)\n",
    "  actions = np.array(actions)\n",
    "  masks = np.array(masks)\n",
    "  return obses, actions, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_obses = None\n",
    "demo_actions = None\n",
    "demo_masks = None\n",
    "train_idxes = None\n",
    "val_batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_demo_rollouts(demo_rollouts):\n",
    "  global demo_obses\n",
    "  global demo_actions\n",
    "  global demo_masks\n",
    "  global train_idxes\n",
    "  global val_batch\n",
    "  \n",
    "  vectorized_demo_rollouts = vectorize_rollouts(demo_rollouts)\n",
    "\n",
    "  demo_obses, demo_actions, demo_masks = vectorized_demo_rollouts\n",
    "  demo_idxes = list(range(demo_obses.shape[0]))\n",
    "\n",
    "  random.shuffle(demo_idxes)\n",
    "  n_train_examples = int(0.9 * len(demo_idxes))\n",
    "  train_idxes = demo_idxes[:n_train_examples]\n",
    "  val_idxes = demo_idxes[n_train_examples:]\n",
    "  val_batch = demo_obses[val_idxes], demo_actions[val_idxes], demo_masks[val_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_demo_rollouts(demo_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_obses.shape, demo_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_rollouts(): # DAgger step\n",
    "  global demo_rollouts\n",
    "  rollouts = []\n",
    "  for oracle_policy, env in zip(oracle_policies, envs):\n",
    "    if human_user:\n",
    "      input('Hit ENTER to begin %d episodes...' % n_agg_rollouts)\n",
    "      time.sleep(5)\n",
    "    for _ in range(n_agg_rollouts):\n",
    "      rollouts.append(label_actions(run_ep(\n",
    "        trained_decoder_policy, env, render=human_user, \n",
    "        blending=dagger_blending, human_user=human_user), oracle_policy))\n",
    "  demo_rollouts += rollouts\n",
    "  process_demo_rollouts(demo_rollouts)\n",
    "  \n",
    "  global dagger_blending\n",
    "  # dynamically adjust blending coeff\n",
    "  dagger_blending = 1 - np.mean([1 if is_succ(rollout) else 0 for rollout in rollouts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(size):\n",
    "  idxes = random.sample(train_idxes, size)\n",
    "  batch = demo_obses[idxes], demo_actions[idxes], demo_masks[idxes]\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 100000\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# RNN hidden layer size\n",
    "num_hidden = 256\n",
    "\n",
    "val_update_freq = 100 # how frequently to evaluate trained decoder on validation env\n",
    "n_val_eval_rollouts = 10 # number of rollouts in validation env\n",
    "\n",
    "# DAgger params\n",
    "agg_freq = 100\n",
    "n_agg_rollouts = 10 # number of rollouts to aggregate into dataset per iteration of DAgger\n",
    "dagger_blending = 0.75 # initial blending coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'imi_decoder_scope.pkl'), 'rb') as f:\n",
    "  imi_decoder_scope = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imi_decoder_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_ph = tf.placeholder(tf.float32, [None, history_len, n_obs_dim]) # observations\n",
    "act_ph = tf.placeholder(tf.float32, [None, history_len, n_act_dim]) # actions\n",
    "mask_ph = tf.placeholder(tf.float32, [None, history_len]) # masks for RNN training\n",
    "init_state_a_ph = tf.placeholder(tf.float32, [None, num_hidden]) # initial state for RNN training\n",
    "init_state_b_ph = tf.placeholder(tf.float32, [None, num_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(imi_decoder_scope, reuse=tf.AUTO_REUSE):\n",
    "  weights = {'out': tf.Variable(tf.random_normal([num_hidden, n_act_dim]))}\n",
    "  biases = {'out': tf.Variable(tf.random_normal([n_act_dim]))}\n",
    "\n",
    "  unstacked_X = tf.unstack(obs_ph, history_len, 1)\n",
    "\n",
    "  lstm_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)\n",
    "\n",
    "  state = (init_state_a_ph, init_state_b_ph)\n",
    "  rnn_outputs = []\n",
    "  rnn_states = []\n",
    "  for input_ in unstacked_X:\n",
    "    output, state = lstm_cell(input_, state)\n",
    "    rnn_outputs.append(tf.matmul(output, weights['out']) + biases['out'])\n",
    "    rnn_states.append(state)\n",
    "\n",
    "reshaped_rnn_outputs = tf.reshape(\n",
    "  tf.concat(rnn_outputs, axis=1), \n",
    "  shape=[tf.shape(obs_ph)[0], history_len, n_act_dim]\n",
    ")\n",
    "\n",
    "loss = tf.reduce_sum(tf.reduce_mean((\n",
    "  reshaped_rnn_outputs - act_ph)**2, axis=2) * mask_ph) / tf.reduce_sum(mask_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainedDecoderPolicy(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.hidden_state = None\n",
    "    self.action = None\n",
    "    self.obs_feed = np.zeros((1, history_len, n_obs_dim))\n",
    "    \n",
    "  def reset(self):\n",
    "    self.hidden_state = (np.zeros((1, num_hidden)), np.zeros((1, num_hidden)))\n",
    "    self.action = None\n",
    "    \n",
    "  def _feed_dict(self, obs):\n",
    "    self.obs_feed[0, 0, :] = obs\n",
    "    return {\n",
    "      init_state_a_ph: self.hidden_state[0],\n",
    "      init_state_b_ph: self.hidden_state[1],\n",
    "      obs_ph: self.obs_feed\n",
    "    }\n",
    "    \n",
    "  def observe(self, obs):\n",
    "    with tf.variable_scope(imi_decoder_scope, reuse=tf.AUTO_REUSE):\n",
    "      self.action, hidden_state = sess.run(\n",
    "        [rnn_outputs[0], rnn_states[0]], feed_dict=self._feed_dict(obs))\n",
    "    if history_len > 1:\n",
    "      self.hidden_state = hidden_state\n",
    "    \n",
    "  def act(self):\n",
    "    assert self.action.shape[0] == 1\n",
    "    return self.action[0, :]\n",
    "  \n",
    "  def get_hidden_state(self):\n",
    "    assert self.hidden_state.c.shape[0] == 1\n",
    "    return self.hidden_state.c[0, :]\n",
    "  \n",
    "trained_decoder_policy = TrainedDecoderPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_logs = {\n",
    "  'train_loss': [],\n",
    "  'val_loss': [],\n",
    "  'rew': [],\n",
    "  'succ': [],\n",
    "  'ttt': [],\n",
    "  'dfot': [],\n",
    "  'ape': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_batch_loss(batch, step=False, t=None):\n",
    "  batch_obs, batch_act, batch_mask = batch \n",
    "  feed_dict = {\n",
    "    obs_ph: batch_obs,\n",
    "    act_ph: batch_act,\n",
    "    mask_ph: batch_mask,\n",
    "    init_state_a_ph: np.zeros((batch_obs.shape[0], num_hidden)),\n",
    "    init_state_b_ph: np.zeros((batch_obs.shape[0], num_hidden))\n",
    "  }\n",
    "  loss_eval = sess.run(loss, feed_dict=feed_dict)\n",
    "  \n",
    "  if step:\n",
    "    sess.run(update_op, feed_dict=feed_dict)\n",
    "  \n",
    "  d = {'loss': loss_eval}\n",
    "  if not step:\n",
    "    _, val_perf = evaluate_decoder_policy(\n",
    "      trained_decoder_policy, \n",
    "      env=make_env(train_goal=False), \n",
    "      n_rollouts=n_val_eval_rollouts\n",
    "    )\n",
    "    d.update(val_perf)\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_log = None\n",
    "while len(train_logs['train_loss']) < iterations:\n",
    "  batch = sample_batch(batch_size)\n",
    "  \n",
    "  t = len(train_logs['train_loss'])\n",
    "  train_log = compute_batch_loss(batch, step=True, t=t)\n",
    "  if val_log is None or t % val_update_freq == 0:\n",
    "    val_log = compute_batch_loss(val_batch, step=False, t=t)\n",
    "    \n",
    "  if t % agg_freq == 0:\n",
    "    aggregate_rollouts()\n",
    "  \n",
    "  print('%d %d %f %f %f %f %f %f %f' % (\n",
    "    t, iterations, train_log['loss'], val_log['loss'], \n",
    "    val_log['rew'], val_log['succ'], val_log['ttt'], val_log['dfot'], val_log['ape']))\n",
    "  \n",
    "  for k, v in train_log.items():\n",
    "    train_logs['%s%s' % ('train_' if k == 'loss' else '', k)].append(v)\n",
    "  for k, v in val_log.items():\n",
    "    train_logs['%s%s' % ('val_' if k == 'loss' else '', k)].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.plot(train_logs['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Reward')\n",
    "plt.axhline(y=oracle_perf['rew'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['rew'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['rew'], color='orange', label='Trained')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.axhline(y=oracle_perf['succ'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['succ'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['succ'], color='orange', label='Trained')\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Time to Target')\n",
    "plt.axhline(y=oracle_perf['ttt'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['ttt'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['ttt'], color='orange', label='Trained')\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Deviation from Optimal Trajectory')\n",
    "plt.axhline(y=oracle_perf['dfot'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['dfot'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['dfot'], color='orange', label='Trained')\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Action Prediction Error')\n",
    "plt.axhline(y=oracle_perf['ape'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['ape'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['ape'], color='orange', label='Trained')\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'train_logs.pkl'), 'wb') as f:\n",
    "  pickle.dump(train_logs, f, pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "with open(os.path.join(data_dir, 'agg_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(demo_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'train_logs.pkl'), 'rb') as f:\n",
    "  train_logs = pickle.load(f)\n",
    "  \n",
    "with open(os.path.join(data_dir, 'agg_rollouts.pkl'), 'rb') as f:\n",
    "  agg_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_viz_rollouts = 50\n",
    "\n",
    "center_goal = np.array([0.5, 0.5])\n",
    "viz_env = CursorControl(goal=center_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_rollouts, _ = evaluate_decoder_policy(\n",
    "  rand_decoder_policy, env=viz_env, n_rollouts=n_viz_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_rollouts_sample = random.sample(rand_rollouts, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_trajectories(rand_rollouts_sample, center_goal, 'Random', 'rand-traj.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_rollouts, _ = evaluate_decoder_policy(\n",
    "  trained_decoder_policy, env=viz_env, n_rollouts=n_viz_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_rollouts_sample = random.sample(trained_rollouts, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_trajectories(trained_rollouts_sample, center_goal, 'Trained', 'trained-traj.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'imi_decoder_scope.pkl'), 'wb') as f:\n",
    "  pickle.dump(imi_decoder_scope, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tf_vars(imi_decoder_scope, os.path.join(data_dir, 'imi_decoder.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tf_vars(imi_decoder_scope, os.path.join(data_dir, 'imi_decoder.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollout = run_ep(trained_decoder_policy, viz_env, render=True, human_user=human_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viz_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tune decoder with q-learning + hard-coded reward function (simulating user feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_build_act(make_obs_ph, q_func, mu_func, scope='deepq', reuse=tf.AUTO_REUSE):\n",
    "  with tf.variable_scope(scope, reuse=reuse):\n",
    "    observations_ph = U.ensure_tf_input(make_obs_ph('observation'))\n",
    "    pilot_action_ph = tf.placeholder(tf.float32, [None, n_act_dim], name='pilot_action')\n",
    "    pilot_tol_ph = tf.placeholder(tf.float32, (), name='pilot_tol')\n",
    "\n",
    "    opt_actions = mu_func(observations_ph.get(), scope=scope, reuse=tf.AUTO_REUSE)\n",
    "    actions = pilot_tol_ph * pilot_action_ph + (1 - pilot_tol_ph) * opt_actions\n",
    "\n",
    "    act = U.function(inputs=[observations_ph, pilot_action_ph, pilot_tol_ph], outputs=[actions])\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_build_train(\n",
    "  make_obs_ph, q_func, mu_func, v_func, optimizer, grad_norm_clipping=None, gamma=1.0,\n",
    "  double_q=True, scope='deepq', reuse=tf.AUTO_REUSE):\n",
    "  \n",
    "  act_f = co_build_act(make_obs_ph, q_func, mu_func, scope=scope, reuse=reuse)\n",
    "\n",
    "  with tf.variable_scope(scope, reuse=reuse):\n",
    "    # set up placeholders\n",
    "    obs_t_input = U.ensure_tf_input(make_obs_ph('obs_t'))\n",
    "    act_t_ph = tf.placeholder(tf.float32, [None, n_act_dim], name='action')\n",
    "    rew_t_ph = tf.placeholder(tf.float32, [None], name='reward')\n",
    "    obs_tp1_input = U.ensure_tf_input(make_obs_ph('obs_tp1'))\n",
    "    done_mask_ph = tf.placeholder(tf.float32, [None], name='done')\n",
    "    importance_weights_ph = tf.placeholder(tf.float32, [None], name='weight')\n",
    "\n",
    "    obs_t_input_get = obs_t_input.get()\n",
    "    obs_tp1_input_get = obs_tp1_input.get()\n",
    "\n",
    "    # q network evaluation\n",
    "    q_t_selected = q_func(obs_t_input_get, act_t_ph, scope='q_func', reuse=reuse)  # reuse parameters from act\n",
    "    q_func_vars = U.scope_vars(U.absolute_scope_name('q_func'))\n",
    "\n",
    "    # compute estimate of best possible value starting from state at t + 1\n",
    "    if double_q:\n",
    "      q_tp1_best_using_online_net = mu_func(obs_tp1_input_get, scope='q_func/mu_func', reuse=reuse)\n",
    "      q_tp1_best = q_func(obs_tp1_input_get, q_tp1_best_using_online_net, scope='target_q_func')\n",
    "    else:\n",
    "      q_tp1_best = v_func(obs_tp1_input_get, scope='target_q_func/v_func')\n",
    "    q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best\n",
    "    \n",
    "    target_q_func_vars = U.scope_vars(U.absolute_scope_name('target_q_func'))\n",
    "    \n",
    "    # compute RHS of bellman equation\n",
    "    q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked\n",
    "\n",
    "    # compute the error (potentially clipped)\n",
    "    td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)\n",
    "    errors = U.huber_loss(td_error)\n",
    "    weighted_error = tf.reduce_mean(importance_weights_ph * errors)\n",
    "\n",
    "    # compute optimization op (potentially with gradient clipping)\n",
    "    if grad_norm_clipping is not None:\n",
    "      optimize_expr = U.minimize_and_clip(\n",
    "        optimizer,\n",
    "        weighted_error,\n",
    "        var_list=q_func_vars,\n",
    "        clip_val=grad_norm_clipping\n",
    "      )\n",
    "    else:\n",
    "        optimize_expr = optimizer.minimize(weighted_error, var_list=q_func_vars)\n",
    "\n",
    "    # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "    update_target_expr = []\n",
    "    for var, var_target in zip(\n",
    "      sorted(q_func_vars, key=lambda v: v.name),\n",
    "      sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "      update_target_expr.append(var_target.assign(var))\n",
    "    update_target_expr = tf.group(*update_target_expr)\n",
    "\n",
    "    # Create callable functions\n",
    "    train = U.function(\n",
    "      inputs=[\n",
    "        obs_t_input,\n",
    "        act_t_ph,\n",
    "        rew_t_ph,\n",
    "        obs_tp1_input,\n",
    "        done_mask_ph,\n",
    "        importance_weights_ph\n",
    "      ],\n",
    "      outputs=td_error,\n",
    "      updates=[optimize_expr]\n",
    "    )\n",
    "    update_target = U.function([], [], updates=[update_target_expr])\n",
    "\n",
    "    q_values = U.function([obs_t_input], q_t_selected)\n",
    "\n",
    "  return act_f, train, update_target, {'q_values': q_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_dqn_learn(\n",
    "  env,\n",
    "  q_func,\n",
    "  mu_func,\n",
    "  v_func,\n",
    "  lr=1e-3,\n",
    "  max_timesteps=100000,\n",
    "  buffer_size=50000,\n",
    "  train_freq=1,\n",
    "  batch_size=32,\n",
    "  print_freq=1,\n",
    "  checkpoint_freq=10000,\n",
    "  learning_starts=1000,\n",
    "  gamma=1.0,\n",
    "  target_network_update_freq=500,\n",
    "  exploration_fraction=0.1,\n",
    "  exploration_final_eps=0.02,\n",
    "  num_cpu=5,\n",
    "  callback=None,\n",
    "  scope='deepq',\n",
    "  pilot_tol=0,\n",
    "  pilot_is_human=False,\n",
    "  reuse=tf.AUTO_REUSE,\n",
    "  buff_init_rollouts=None):\n",
    "\n",
    "  # Create all the functions necessary to train the model\n",
    "\n",
    "  sess = U.get_session()\n",
    "  if sess is None:\n",
    "    sess = U.make_session(num_cpu=num_cpu)\n",
    "    sess.__enter__()\n",
    "\n",
    "  def make_obs_ph(name):\n",
    "    return U.BatchInput(env.observation_space.shape, name=name)\n",
    "\n",
    "  act, train, update_target, debug = co_build_train(\n",
    "    scope=scope,\n",
    "    make_obs_ph=make_obs_ph,\n",
    "    q_func=q_func,\n",
    "    mu_func=mu_func,\n",
    "    v_func=v_func,\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
    "    gamma=gamma,\n",
    "    grad_norm_clipping=10,\n",
    "    reuse=reuse\n",
    "  )\n",
    "\n",
    "  act_params = {\n",
    "    'make_obs_ph': make_obs_ph,\n",
    "    'q_func': q_func,\n",
    "    'mu_func': mu_func,\n",
    "    'v_func': v_func\n",
    "  }\n",
    "\n",
    "  replay_buffer = ReplayBuffer(buffer_size)\n",
    "  \n",
    "  if buff_init_rollouts is not None:\n",
    "    for rollout in buff_init_rollouts:\n",
    "      for obs, _, rew, new_obs, done, info in rollout:\n",
    "        action = info['action_taken']\n",
    "        replay_buffer.add(obs, info['action_taken'], rew, new_obs, done)\n",
    "\n",
    "  # Initialize the parameters and copy them to the target network.\n",
    "  U.initialize()\n",
    "  update_target()\n",
    "\n",
    "  episode_rewards = [0.0]\n",
    "  episode_outcomes = []\n",
    "  saved_mean_reward = None\n",
    "  obs = env.reset()\n",
    "  prev_t = 0\n",
    "  rollouts = []\n",
    "\n",
    "  if pilot_is_human:\n",
    "    global human_agent_action\n",
    "    global human_agent_active\n",
    "    human_agent_action = init_human_action()\n",
    "    human_agent_active = False\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as td:\n",
    "    model_saved = False\n",
    "    model_file = os.path.join(td, 'model')\n",
    "    for t in range(max_timesteps):\n",
    "      act_kwargs = {\n",
    "        'pilot_action': extract_pilot_action(obs)[None, :],\n",
    "        'pilot_tol': pilot_tol if not pilot_is_human or (pilot_is_human and human_agent_active) else 0\n",
    "      }\n",
    "\n",
    "      action = act(obs[None, :], **act_kwargs)[0][0]\n",
    "      new_obs, rew, done, info = env.step(action)\n",
    "\n",
    "      if pilot_is_human:\n",
    "        env.render()\n",
    "        time.sleep(delay_between_steps_for_human)\n",
    "\n",
    "      # Store transition in the replay buffer.\n",
    "      replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "      obs = new_obs\n",
    "\n",
    "      episode_rewards[-1] += rew\n",
    "\n",
    "      if done:\n",
    "        if t > learning_starts:\n",
    "          for _ in range(t - prev_t):\n",
    "            obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
    "            weights, batch_idxes = np.ones_like(rewards), None\n",
    "            td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        episode_outcomes.append(info)\n",
    "        episode_rewards.append(0.0)\n",
    "\n",
    "        if pilot_is_human:\n",
    "          global human_agent_action\n",
    "          human_agent_action = init_human_action()\n",
    "\n",
    "        prev_t = t\n",
    "\n",
    "        if pilot_is_human:\n",
    "          time.sleep(delay_between_episodes_for_human)\n",
    "\n",
    "      if t > learning_starts and t % target_network_update_freq == 0:\n",
    "        # Update target network periodically.\n",
    "        update_target()\n",
    "\n",
    "      mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
    "      mean_100ep_succ = round(np.mean([1 if x['succ'] else 0 for x in episode_outcomes[-101:-1]]), 2)\n",
    "      mean_100ep_ttt = round(np.mean([x['ttt'] for x in episode_outcomes[-101:-1]]), 2)\n",
    "      mean_100ep_dfot = round(np.mean([x['dfot'] for x in episode_outcomes[-101:-1]]), 2)\n",
    "      num_episodes = len(episode_rewards)\n",
    "      if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n",
    "        logger.record_tabular('steps', t)\n",
    "        logger.record_tabular('episodes', num_episodes)\n",
    "        logger.record_tabular('mean 100 episode reward', mean_100ep_reward)\n",
    "        logger.record_tabular('mean 100 episode succ', mean_100ep_succ)\n",
    "        logger.record_tabular('mean 100 episode ttt', mean_100ep_ttt)\n",
    "        logger.record_tabular('mean 100 episode dfot', mean_100ep_dfot)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "      if checkpoint_freq is not None and t > learning_starts and num_episodes > 100 and t % checkpoint_freq == 0 and (saved_mean_reward is None or mean_100ep_reward > saved_mean_reward):\n",
    "        if print_freq is not None:\n",
    "          print('Saving model due to mean reward increase:')\n",
    "          print(saved_mean_reward, mean_100ep_reward)\n",
    "        U.save_state(model_file)\n",
    "        model_saved = True\n",
    "        saved_mean_reward = mean_100ep_reward\n",
    "\n",
    "    if model_saved:\n",
    "      U.load_state(model_file)\n",
    "\n",
    "  reward_data = {\n",
    "    'rewards': episode_rewards,\n",
    "    'outcomes': episode_outcomes\n",
    "  }\n",
    "\n",
    "  return ActWrapper(act, act_params), reward_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_co_policy(\n",
    "  env, pilot_tol, pilot_is_human, \n",
    "  n_training_episodes, copilot_scope, copilot_q_func, \n",
    "  copilot_mu_func, copilot_v_func, buff_init_rollouts, reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "  return co_dqn_learn(\n",
    "    env,\n",
    "    scope=copilot_scope,\n",
    "    q_func=copilot_q_func,\n",
    "    mu_func=copilot_mu_func,\n",
    "    v_func=copilot_v_func,\n",
    "    max_timesteps=max_ep_len*n_training_episodes,\n",
    "    pilot_tol=pilot_tol,\n",
    "    pilot_is_human=pilot_is_human,\n",
    "    reuse=reuse,\n",
    "    buff_init_rollouts=buff_init_rollouts,\n",
    "    **copilot_dqn_learn_kwargs\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delay_between_episodes_for_human = 1\n",
    "delay_between_steps_for_human = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pilot_tol = 0.5 # blending coefficient\n",
    "# 1 -> full \"pilot\", i.e., imitation-learned decoder\n",
    "# 0 -> full \"copilot\", i.e., Q-learned decoder\n",
    "\n",
    "pilot_policy = trained_decoder_policy\n",
    "# we will be learning to assist `trained_decoder_policy`, \n",
    "# which was trained earlier with imitation learning and is now a fixed decoder\n",
    "\n",
    "buff_init_rollouts = agg_rollouts\n",
    "# initialize replay buffer with rollouts collected during imitation learning\n",
    "\n",
    "using_reward_shaping = False\n",
    "# WARNING: make sure this is consistent with the rewards in buff_init_rollouts\n",
    "\n",
    "return_to_init = False # True -> need to return to initial position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augment_obs = lambda obs, pilot_hidden_state, pilot_action: np.concatenate((obs, pilot_hidden_state, pilot_action))\n",
    "extract_pilot_action = lambda obs: obs[-n_act_dim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if buff_init_rollouts is not None:\n",
    "  for i, rollout in enumerate(buff_init_rollouts):\n",
    "    pilot_policy.reset()\n",
    "    aug_obses = []\n",
    "    unzipped_rollout = [list(x) for x in zip(*rollout)]\n",
    "    obses = unzipped_rollout[0]\n",
    "    next_obses = unzipped_rollout[3]\n",
    "    obses.append(next_obses[-1])\n",
    "    for obs in obses:\n",
    "      pilot_policy.observe(obs)\n",
    "      pilot_action = pilot_policy.act()\n",
    "      pilot_hidden_state = pilot_policy.get_hidden_state()\n",
    "      aug_obses.append(augment_obs(obs, pilot_hidden_state, pilot_action))\n",
    "    aug_next_obses = aug_obses[1:]\n",
    "    aug_obses = aug_obses[:-1]\n",
    "    unzipped_rollout[0] = aug_obses\n",
    "    unzipped_rollout[3] = aug_next_obses\n",
    "    buff_init_rollouts[i] = zip(*unzipped_rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_training_episodes = 500\n",
    "\n",
    "copilot_mu_func = lambda obs_ph, scope, reuse: deepq.models._mlp(\n",
    "  [64], obs_ph, n_act_dim, scope=scope+'/mu_func', reuse=reuse)\n",
    "\n",
    "copilot_v_func = lambda obs_ph, scope, reuse: deepq.models._mlp(\n",
    "  [64], obs_ph, 1, scope=scope+'/v_func', reuse=reuse)\n",
    "\n",
    "copilot_l_func = lambda obs_ph, scope, reuse: deepq.models._mlp(\n",
    "  [64], obs_ph, 1, scope=scope+'/l_func', reuse=reuse)\n",
    "\n",
    "def copilot_q_func(obs_ph, act_ph, scope, reuse=tf.AUTO_REUSE): # NAF\n",
    "  opt_act = copilot_mu_func(obs_ph, scope=scope, reuse=reuse)\n",
    "  adv_std = copilot_l_func(obs_ph, scope=scope, reuse=reuse)\n",
    "  A = -tf.einsum('ij,ij->i', act_ph - opt_act, act_ph - opt_act) * (adv_std**2)\n",
    "  V = copilot_v_func(obs_ph, scope=scope, reuse=reuse)\n",
    "  return A + V\n",
    "\n",
    "copilot_dqn_learn_kwargs = {\n",
    "  'lr': 1e-3,\n",
    "  'exploration_fraction': 0.1,\n",
    "  'exploration_final_eps': 0.02,\n",
    "  'target_network_update_freq': 1500,\n",
    "  'print_freq': 100,\n",
    "  'num_cpu': 5,\n",
    "  'gamma': 0.99,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copilot_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CursorControl(\n",
    "  rand_goal=True, # new, random goal for each episode\n",
    "  human_user=human_user, return_to_init=return_to_init, \n",
    "  using_reward_shaping=using_reward_shaping, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.unwrapped.observation_space = spaces.Box(\n",
    "  np.zeros(n_obs_dim + num_hidden + n_act_dim), \n",
    "  np.ones(n_obs_dim + num_hidden + n_act_dim))\n",
    "\n",
    "env.unwrapped.pilot_policy = pilot_policy\n",
    "\n",
    "def _obs(self):\n",
    "  goal_reached_ind = np.array([1.0 if self.goal_reached else 0.0])\n",
    "  ext_obs = np.concatenate((self.pos, self.vel, self.init_pos, goal_reached_ind)) # external state observations (\"context\")\n",
    "  int_act = self.user_policy(ext_obs) # intended action\n",
    "  bci_obs = internal_encode_obs(int_act, ext_obs, self.goal) # BCI output\n",
    "  self.curr_obs = np.concatenate((ext_obs, bci_obs))\n",
    "  self.pilot_policy.observe(self.curr_obs) # WARNING: side effect\n",
    "  self.curr_obs = np.concatenate((self.curr_obs, self.pilot_policy.get_hidden_state(), self.pilot_policy.act()))\n",
    "  return self.curr_obs\n",
    "env.unwrapped._obs = types.MethodType(_obs, env.unwrapped)\n",
    "\n",
    "env.unwrapped._reset_orig = env.unwrapped._reset\n",
    "def _reset(self):\n",
    "  self.pilot_policy.reset()\n",
    "  return self._reset_orig()\n",
    "env.unwrapped._reset = types.MethodType(_reset, env.unwrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_copilot_policy, reward_data = make_co_policy(\n",
    "  env=env, pilot_tol=pilot_tol, \n",
    "  pilot_is_human=human_user, copilot_scope=copilot_scope, \n",
    "  copilot_q_func=copilot_q_func, copilot_mu_func=copilot_mu_func,\n",
    "  copilot_v_func=copilot_v_func, n_training_episodes=n_training_episodes,\n",
    "  buff_init_rollouts=buff_init_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tf_vars(copilot_scope, os.path.join(data_dir, 'copilot.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tf_vars(copilot_scope, os.path.join(data_dir, 'copilot.tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'copilot_training_data.pkl'), 'wb') as f:\n",
    "  pickle.dump((copilot_scope, reward_data), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'copilot_training_data.pkl'), 'wb') as f:\n",
    "  copilot_scope, reward_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copilot_policy(obs):\n",
    "  pilot_action = extract_pilot_action(obs)\n",
    "  with tf.variable_scope(copilot_scope, reuse=tf.AUTO_REUSE):\n",
    "    return raw_copilot_policy._act(\n",
    "      obs[None, :], pilot_action=pilot_action, pilot_tol=pilot_tol)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollout = run_ep(copilot_policy, env, render=True, human_user=human_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
