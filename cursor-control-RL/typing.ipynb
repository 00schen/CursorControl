{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.classic_control import rendering\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyglet.window import key as pygkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rc('savefig', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', 'typing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = 10 # each task/env has a unique goal/string\n",
    "goal_len = 10\n",
    "\n",
    "n_act_dim = 97 # number of discrete actions (ascii chars 32-127 + noop)\n",
    "grid_size = (28, 32) # ecog grid dimensions\n",
    "bci_dim = grid_size[0]*grid_size[1] # number of BCI channels\n",
    "n_ext_obs_dim = n_act_dim # number of external state observation dimensions\n",
    "n_obs_dim = n_ext_obs_dim + bci_dim\n",
    "\n",
    "gamma = 0.99 # discount factor\n",
    "max_ep_len = 100 # number of timesteps\n",
    "succ_rew_bonus = 1 # for reaching goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_succ = lambda r: r[-1][-1]['succ']\n",
    "get_ttt = lambda r: r[-1][-1]['ttt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_goal():\n",
    "  return ''.join(chr(np.random.randint(32, 127)) for _ in range(goal_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_dir = os.environ['GPT2_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(gpt2_dir, 'src'))\n",
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '117M'\n",
    "\n",
    "enc = encoder.get_encoder(model_name)\n",
    "hparams = model.default_hparams()\n",
    "with open(os.path.join(gpt2_dir, 'models', model_name, 'hparams.json')) as f:\n",
    "  hparams.override_from_dict(json.load(f))\n",
    "  \n",
    "context = tf.placeholder(tf.int32, [1, None])\n",
    "\n",
    "lm_samp = sample.sample_sequence(\n",
    "  hparams=hparams, length=(goal_len*10),\n",
    "  start_token=enc.encoder['<|endoftext|>'],\n",
    "  batch_size=1,\n",
    "  temperature=1, top_k=0\n",
    ")[:, 1:]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.latest_checkpoint(os.path.join(gpt2_dir, 'models', model_name))\n",
    "saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_char = {enc.decode([i]): i for i in range(hparams.n_vocab)}\n",
    "valid_idxes = [idx_of_char[chr(i)] for i in range(32, 127)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_logits = lambda x: np.log(np.exp(x) / np.sum(np.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_prior(curr_string):\n",
    "  if curr_string == '':\n",
    "    curr_string = '<|endoftext|>'\n",
    "    \n",
    "  context_tokens = enc.encode(curr_string)\n",
    "\n",
    "  lm_output = model.model(hparams=hparams, X=context, past=None, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "  feed_dict = {context: [context_tokens]}\n",
    "  logits = sess.run(lm_output, feed_dict=feed_dict)\n",
    "  logits = logits['logits'][0, -1, :hparams.n_vocab]\n",
    "\n",
    "  return normalize_logits(logits[valid_idxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_goal():\n",
    "  filtered = None\n",
    "  while filtered is None or len(filtered) < goal_len:\n",
    "    raw = enc.decode(sess.run(lm_samp)[0])\n",
    "    filtered = ''.join([c for c in raw if ord(c) >= 32 and ord(c) < 127])\n",
    "  return filtered[:goal_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = [sample_random_goal() for _ in range(n_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'goals.pkl'), 'wb') as f:\n",
    "  pickle.dump(goals, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'goals.pkl'), 'rb') as f:\n",
    "  goals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(i, n):\n",
    "  x = np.zeros(n)\n",
    "  x[i] = 1\n",
    "  return x\n",
    "\n",
    "onehot_decode = lambda x: np.nonzero(x)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(D):\n",
    "  decode_act = lambda obs: sample_act(D.dot(extract_bci_obs(obs)))\n",
    "  return decode_act\n",
    "\n",
    "def make_encoder(D):\n",
    "  D_inv = np.linalg.pinv(D)\n",
    "  encode_obs = lambda action: D_inv.dot(onehot_encode(action, n_act_dim))\n",
    "  return encode_obs\n",
    "\n",
    "# simulate user with fixed, random projection \n",
    "# from intended action distribution to BCI output\n",
    "D_int = np.random.random((n_act_dim, bci_dim))\n",
    "internal_decode_act = make_decoder(D_int)\n",
    "internal_encode_obs = make_encoder(D_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOOP = 96\n",
    "BACK = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Typing(gym.Env):\n",
    "  metadata = {\n",
    "    'render.modes': ['human']\n",
    "  }\n",
    "  \n",
    "  def __init__(\n",
    "      self, \n",
    "      max_ep_len=max_ep_len,\n",
    "      goal=None,\n",
    "      rand_goal=False,\n",
    "      using_reward_shaping=True,\n",
    "      blending=0 # between 0 and 1 (0 -> no blending, 1 -> ignore agent and take optimal actions)\n",
    "    ):\n",
    "    \n",
    "    lows = np.zeros(n_obs_dim)\n",
    "    highs = np.ones(n_obs_dim)\n",
    "      \n",
    "    self.observation_space = spaces.Box(lows, highs)\n",
    "    self.action_space = spaces.Discrete(n_act_dim)\n",
    "    \n",
    "    self.curr_string = None\n",
    "    self.curr_step = None\n",
    "    self.viewer = None\n",
    "    self.curr_obs = None\n",
    "    self.succ = None\n",
    "    self.prev_action = None\n",
    "    \n",
    "    self.succ_rew_bonus = succ_rew_bonus\n",
    "    self.max_ep_len = max_ep_len\n",
    "    self.goal = goal\n",
    "    self.blending = blending\n",
    "    self.using_reward_shaping = using_reward_shaping\n",
    "    self.rand_goal = rand_goal\n",
    "    \n",
    "    if not rand_goal:\n",
    "      assert goal is not None\n",
    "      self._set_goal(goal)\n",
    "      \n",
    "  def _set_goal(self, goal):\n",
    "    self.goal = goal\n",
    "    self.optimal_user_policy = make_synth_user_policy(goal, using_ext_obs=True)\n",
    "    self.user_policy = make_synth_user_policy(goal, using_ext_obs=True)\n",
    "    self.reward_func = make_reward_func(goal, using_reward_shaping=self.using_reward_shaping)\n",
    "    \n",
    "  def _ext_obs(self):\n",
    "    return onehot_encode(self.prev_action, n_act_dim) # context\n",
    "        \n",
    "  def _obs(self):\n",
    "    ext_obs = self._ext_obs()\n",
    "    int_act = self.user_policy(ext_obs) # intended action\n",
    "    bci_obs = internal_encode_obs(int_act) # BCI output\n",
    "    self.curr_obs = np.concatenate((ext_obs, bci_obs))\n",
    "    return self.curr_obs\n",
    "  \n",
    "  def _state(self):\n",
    "    return self.curr_string\n",
    "\n",
    "  def _step(self, action):  \n",
    "    if np.random.random() < self.blending:\n",
    "      action = self.optimal_user_policy(self._ext_obs())\n",
    "            \n",
    "    if action == NOOP:\n",
    "      pass\n",
    "    elif action == BACK:\n",
    "      if len(self.curr_string) > 0:\n",
    "        self.curr_string = self.curr_string[:-1]\n",
    "    elif 0 <= action and action < 95:\n",
    "      self.curr_string += chr(32+action)\n",
    "    else:\n",
    "      raise ValueError('invalid action')\n",
    "              \n",
    "    self.curr_step += 1\n",
    "    self.succ = self.curr_string == self.goal\n",
    "    oot = self.curr_step >= self.max_ep_len # out of time\n",
    "    \n",
    "    self.prev_action = action\n",
    "    \n",
    "    ext_obs = self._ext_obs()\n",
    "    self.optimal_user_policy.observe(ext_obs)\n",
    "    self.user_policy.observe(ext_obs)\n",
    "    \n",
    "    obs = self._obs()\n",
    "        \n",
    "    state = self._state()\n",
    "    r = self.reward_func(self.prev_state, action, state)\n",
    "    done = oot or self.succ\n",
    "    info = {'goal': self.goal, 'succ': self.succ, 'ttt': self.curr_step}\n",
    "      \n",
    "    self.prev_state = state\n",
    "    self.prev_obs = obs\n",
    "        \n",
    "    return obs, r, done, info\n",
    "    \n",
    "  def _reset(self):\n",
    "    self.curr_string = ''\n",
    "    self.succ = False\n",
    "    self.prev_action = NOOP\n",
    "    self.optimal_user_policy.reset()\n",
    "    self.user_policy.reset()\n",
    "      \n",
    "    self.curr_step = 0\n",
    "    \n",
    "    if self.rand_goal:\n",
    "      self._set_goal(sample_random_goal())\n",
    "    \n",
    "    ext_obs = self._ext_obs()\n",
    "    self.optimal_user_policy.observe(ext_obs)\n",
    "    self.user_policy.observe(ext_obs)\n",
    "    self.prev_obs = self._obs()\n",
    "    \n",
    "    self.prev_state = self._state()\n",
    "    return self.prev_obs\n",
    "  \n",
    "  def _render(self, mode='human', close=False):\n",
    "    if close:\n",
    "      if self.viewer is not None:\n",
    "        self.viewer.close()\n",
    "        self.viewer = None\n",
    "      return\n",
    "    \n",
    "    if self.viewer is None:\n",
    "      self.viewer = rendering.SimpleImageViewer()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    canvas = FigureCanvas(fig)\n",
    "    \n",
    "    plt.text(0.01, 0.5, self.curr_string)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    agg = canvas.switch_backends(FigureCanvas)\n",
    "    agg.draw()\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    self.viewer.imshow(\n",
    "      np.fromstring(agg.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0 # deterministic\n",
    "sample_act = lambda logits: np.argmax(\n",
    "  logits + temperature * np.random.gumbel(0, 1, logits.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_bci_obs = lambda obs: obs[n_ext_obs_dim:]\n",
    "extract_ext_obs = lambda obs: obs[:n_ext_obs_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_common_prefix(a, b):\n",
    "  n = min(len(a), len(b))\n",
    "  for i in range(n):\n",
    "    if a[i] != b[i]:\n",
    "      return i\n",
    "  return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate user with optimal intended actions\n",
    "class SynthUser(object):\n",
    "  \n",
    "  def __init__(self, goal, using_ext_obs=False):\n",
    "    self.goal = goal\n",
    "    self.using_ext_obs = using_ext_obs\n",
    "    self.curr_string = ''\n",
    "    \n",
    "  def reset(self):\n",
    "    self.curr_string = ''\n",
    "    \n",
    "  def observe(self, obs):\n",
    "    if not self.using_ext_obs:\n",
    "      obs = extract_ext_obs(obs)\n",
    "    prev_action = onehot_decode(obs)\n",
    "    if prev_action == NOOP:\n",
    "      pass\n",
    "    elif prev_action == BACK:\n",
    "      if len(self.curr_string) > 0:\n",
    "        self.curr_string = self.curr_string[:-1]\n",
    "    elif 0 <= prev_action and prev_action < 95:\n",
    "      self.curr_string += chr(32+prev_action)\n",
    "    \n",
    "  def __call__(self, obs):\n",
    "    if len_common_prefix(self.curr_string, self.goal) < len(self.curr_string):\n",
    "      return BACK\n",
    "    else:\n",
    "      if len(self.curr_string) < len(self.goal):\n",
    "        return ord(self.goal[len(self.curr_string)])-32\n",
    "      else:\n",
    "        return NOOP\n",
    "\n",
    "make_synth_user_policy = SynthUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_func(goal, using_reward_shaping=True):\n",
    "\n",
    "  def reward_shaping(state):\n",
    "    n = len(goal) - len_common_prefix(state, goal)\n",
    "    if len(state) > len(goal):\n",
    "      n += len(state) - len(goal)\n",
    "    return -n # length of shortest path to goal\n",
    "\n",
    "  def reward_func(prev_state, action, state):\n",
    "    if state == goal:\n",
    "      r = succ_rew_bonus\n",
    "    else:\n",
    "      r = 0\n",
    "    if using_reward_shaping:\n",
    "      r += gamma * reward_shaping(state) - reward_shaping(prev_state)\n",
    "    return r\n",
    "  \n",
    "  return reward_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one env/task per goal\n",
    "envs = [Typing(goal=goal) for goal in goals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_ep(policy, env, max_ep_len=max_ep_len, render=False, blending=0):\n",
    "  old_blending = copy(env.blending)\n",
    "  env.blending = blending\n",
    "  \n",
    "  obs = env.reset()\n",
    "  try:\n",
    "    policy.reset()\n",
    "  except:\n",
    "    pass\n",
    "  done = False\n",
    "  prev_obs = obs\n",
    "  rollout = []\n",
    "  \n",
    "  for step_idx in range(max_ep_len+1):\n",
    "    if done:\n",
    "      break\n",
    "    \n",
    "    try:\n",
    "      policy.observe(obs)\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    action = policy(obs)\n",
    "    obs, r, done, info = env.step(action)\n",
    "    \n",
    "    rollout.append((prev_obs, action, r, obs, float(done), info))\n",
    "    prev_obs = obs\n",
    "    if render:\n",
    "      env.render()\n",
    "      \n",
    "  env.blending = old_blending\n",
    "  \n",
    "  if render:\n",
    "    env.close()\n",
    "    \n",
    "  return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_policies = [make_synth_user_policy(env.goal) for env in envs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_decoder_policy = internal_decode_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed, random, linear decoder\n",
    "D_rand = np.random.random((n_act_dim, bci_dim))\n",
    "rand_decode_act = lambda obs: sample_act(D_rand.dot(extract_bci_obs(obs)))\n",
    "rand_decoder_policy = rand_decode_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity-check envs, agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rollout = run_ep(oracle_policies[task_idx], envs[task_idx], render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = run_ep(oracle_decoder_policy, envs[task_idx], render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rollout = run_ep(rand_decoder_policy, envs[task_idx], render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "envs[task_idx].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_rollouts = 100\n",
    "\n",
    "def make_env(train_goal=True):\n",
    "  test_goal = sample_random_goal() if not train_goal else goals[np.random.choice(\n",
    "    list(range(n_tasks)))]\n",
    "  env = Typing(goal=test_goal)\n",
    "  return env\n",
    "\n",
    "def evaluate_decoder_policy(decoder_policy, env=None, n_rollouts=n_eval_rollouts):\n",
    "  if env is None:\n",
    "    env = make_env(train_goal=False)\n",
    "  rollouts = [run_ep(\n",
    "    decoder_policy, env, render=False, blending=0) for _ in range(n_rollouts)]\n",
    "  perf = {\n",
    "    'rew': np.mean([sum(x[2] for x in rollout) for rollout in rollouts]),\n",
    "    'succ': np.mean([1 if is_succ(rollout) else 0 for rollout in rollouts]),\n",
    "    'ttt': np.mean([get_ttt(rollout) for rollout in rollouts if is_succ(rollout)])\n",
    "  }\n",
    "  return rollouts, perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_rollouts, oracle_perf = evaluate_decoder_policy(oracle_decoder_policy, n_rollouts=n_eval_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'oracle_eval.pkl'), 'wb') as f:\n",
    "  pickle.dump((oracle_rollouts, oracle_perf), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'oracle_eval.pkl'), 'rb') as f:\n",
    "  oracle_rollouts, oracle_perf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_rollouts, rand_perf = evaluate_decoder_policy(rand_decoder_policy, n_rollouts=n_eval_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'rand_eval.pkl'), 'wb') as f:\n",
    "  pickle.dump((rand_rollouts, rand_perf), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'rand_eval.pkl'), 'rb') as f:\n",
    "  rand_rollouts, rand_perf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train decoder with imitation learning\n",
    "\n",
    "[Neuroprosthetic decoder training as imitation learning](https://arxiv.org/abs/1511.04156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_demo_rollouts_per_task = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_policies = [rand_decoder_policy for _ in range(n_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_blending = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_actions(rollout, policy):\n",
    "  try:\n",
    "    policy.reset()\n",
    "  except:\n",
    "    pass\n",
    "  for i, x in enumerate(rollout):\n",
    "    x = list(x)\n",
    "    x[-1]['action_taken'] = x[1]\n",
    "    try:\n",
    "      policy.observe(x[0])\n",
    "    except:\n",
    "      pass\n",
    "    x[1] = policy(x[0]) # replace taken action with action label\n",
    "    rollout[i] = tuple(x)\n",
    "  return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_rollouts = [label_actions(run_ep(\n",
    "  demo_policies[task_idx], env, render=False, blending=demo_blending\n",
    "), oracle_policies[task_idx]) for _ in range(\n",
    "  n_demo_rollouts_per_task) for task_idx, env in enumerate(envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'demo_rollouts.pkl'), 'wb') as f:\n",
    "  pickle.dump(demo_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'demo_rollouts.pkl'), 'rb') as f:\n",
    "  demo_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of timesteps into the past \n",
    "# that the RNN decoder can look at\n",
    "history_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_mask(i, n): # for RNN training\n",
    "  x = np.zeros(n)\n",
    "  x[:i] = 1\n",
    "  return x\n",
    "\n",
    "pad_obses = lambda obses, n: list(obses) + [np.zeros(obses[-1].shape)] * (n - len(obses))\n",
    "pad_acts = lambda acts, n: list(acts) + [0] * (n - len(acts))\n",
    "\n",
    "def vectorize_rollouts(rollouts):\n",
    "  obses = []\n",
    "  actions = []\n",
    "  masks = []\n",
    "  for rollout in rollouts:\n",
    "    more_obses, more_actions = list(zip(*rollout))[:2]\n",
    "    for i in range(max(1, len(more_obses)-history_len+1)):\n",
    "      unpadded_obses = more_obses[i:i+history_len]\n",
    "      obses.append(pad_obses(unpadded_obses, history_len))\n",
    "      actions.append(pad_acts(more_actions[i:i+history_len], history_len))\n",
    "      masks.append(build_mask(len(unpadded_obses), history_len))\n",
    "  obses = np.array(obses)\n",
    "  actions = np.array(actions)\n",
    "  masks = np.array(masks)\n",
    "  return obses, actions, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_obses = None\n",
    "demo_actions = None\n",
    "demo_masks = None\n",
    "train_idxes = None\n",
    "val_batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_demo_rollouts(demo_rollouts):\n",
    "  global demo_obses\n",
    "  global demo_actions\n",
    "  global demo_masks\n",
    "  global train_idxes\n",
    "  global val_batch\n",
    "  \n",
    "  vectorized_demo_rollouts = vectorize_rollouts(demo_rollouts)\n",
    "\n",
    "  demo_obses, demo_actions, demo_masks = vectorized_demo_rollouts\n",
    "  demo_idxes = list(range(demo_obses.shape[0]))\n",
    "\n",
    "  random.shuffle(demo_idxes)\n",
    "  n_train_examples = int(0.9 * len(demo_idxes))\n",
    "  train_idxes = demo_idxes[:n_train_examples]\n",
    "  val_idxes = demo_idxes[n_train_examples:]\n",
    "  val_batch = demo_obses[val_idxes], demo_actions[val_idxes], demo_masks[val_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_demo_rollouts(demo_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_obses.shape, demo_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rollouts(): # DAgger step\n",
    "  global demo_rollouts\n",
    "  rollouts = []\n",
    "  for oracle_policy, env in zip(oracle_policies, envs):\n",
    "    for _ in range(n_agg_rollouts):\n",
    "      rollouts.append(label_actions(run_ep(\n",
    "        trained_decoder_policy, env, render=False, \n",
    "        blending=dagger_blending), oracle_policy))\n",
    "  demo_rollouts += rollouts\n",
    "  process_demo_rollouts(demo_rollouts)\n",
    "  \n",
    "  global dagger_blending\n",
    "  # dynamically adjust blending coeff\n",
    "  dagger_blending = 1 - np.mean([1 if is_succ(rollout) else 0 for rollout in rollouts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(size):\n",
    "  idxes = random.sample(train_idxes, size)\n",
    "  batch = demo_obses[idxes], demo_actions[idxes], demo_masks[idxes]\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "    input_placeholder,\n",
    "    output_size,\n",
    "    scope,\n",
    "    n_layers=1,\n",
    "    size=256,\n",
    "    activation=tf.nn.relu,\n",
    "    output_activation=tf.nn.softmax,\n",
    "    reuse=False\n",
    "  ):\n",
    "  out = input_placeholder\n",
    "  with tf.variable_scope(scope, reuse=reuse):\n",
    "    for _ in range(n_layers):\n",
    "      out = tf.layers.dense(out, size, activation=activation)\n",
    "    out = tf.layers.dense(out, output_size, activation=output_activation)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 100000\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# RNN hidden layer size\n",
    "num_hidden = 512\n",
    "\n",
    "val_update_freq = 100 # how frequently to evaluate trained decoder on validation env\n",
    "n_val_eval_rollouts = 10 # number of rollouts in validation env\n",
    "\n",
    "# DAgger params\n",
    "agg_freq = 1000\n",
    "n_agg_rollouts = 1 # number of rollouts to aggregate into dataset per iteration of DAgger\n",
    "dagger_blending = 0.75 # initial blending coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'imi_decoder_scope.pkl'), 'rb') as f:\n",
    "  imi_decoder_scope = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imi_decoder_scope = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_ph = tf.placeholder(tf.float32, [None, history_len, n_obs_dim]) # observations\n",
    "act_ph = tf.placeholder(tf.int32, [None, history_len]) # actions\n",
    "mask_ph = tf.placeholder(tf.float32, [None, history_len]) # masks for RNN training\n",
    "init_state_a_ph = tf.placeholder(tf.float32, [None, num_hidden]) # initial state for RNN training\n",
    "init_state_b_ph = tf.placeholder(tf.float32, [None, num_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(imi_decoder_scope, reuse=tf.AUTO_REUSE):\n",
    "  weights = {'out': tf.Variable(tf.random_normal([num_hidden, n_act_dim]))}\n",
    "  biases = {'out': tf.Variable(tf.random_normal([n_act_dim]))}\n",
    "\n",
    "  unstacked_X = tf.unstack(obs_ph, history_len, 1)\n",
    "\n",
    "  lstm_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)\n",
    "\n",
    "  state = (init_state_a_ph, init_state_b_ph)\n",
    "  rnn_outputs = []\n",
    "  rnn_states = []\n",
    "  for input_ in unstacked_X:\n",
    "    output, state = lstm_cell(input_, state)\n",
    "    rnn_outputs.append(tf.log(\n",
    "      tf.nn.softmax(tf.matmul(output, weights['out']) + biases['out'])))\n",
    "    rnn_states.append(state)\n",
    "\n",
    "act_log_likelihoods = tf.reshape(\n",
    "  tf.concat(rnn_outputs, axis=1), \n",
    "  shape=[tf.shape(obs_ph)[0], history_len, n_act_dim]\n",
    ")\n",
    "\n",
    "selected_act_lls = tf.reduce_sum(\n",
    "  act_log_likelihoods * tf.one_hot(act_ph, n_act_dim, axis=-1), axis=2)\n",
    "\n",
    "loss = -tf.reduce_sum(selected_act_lls * mask_ph) / tf.reduce_sum(mask_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedDecoderPolicy(object):\n",
    "  \n",
    "  def __init__(self, using_prior=True):\n",
    "    self.using_prior = using_prior\n",
    "    self.hidden_state = None\n",
    "    self.action = None\n",
    "    self.curr_string = None\n",
    "    self.obs_feed = np.zeros((1, history_len, n_obs_dim))\n",
    "    \n",
    "  def reset(self):\n",
    "    self.hidden_state = (np.zeros((1, num_hidden)), np.zeros((1, num_hidden)))\n",
    "    self.curr_string = ''\n",
    "    self.action = None\n",
    "    \n",
    "  def _feed_dict(self, obs):\n",
    "    self.obs_feed[0, 0, :] = obs\n",
    "    return {\n",
    "      init_state_a_ph: self.hidden_state[0],\n",
    "      init_state_b_ph: self.hidden_state[1],\n",
    "      obs_ph: self.obs_feed\n",
    "    }\n",
    "    \n",
    "  def observe(self, obs):\n",
    "    with tf.variable_scope(imi_decoder_scope, reuse=tf.AUTO_REUSE):\n",
    "      logits, self.hidden_state = sess.run(\n",
    "        [rnn_outputs[0], rnn_states[0]], feed_dict=self._feed_dict(obs))\n",
    "    logits = logits[0, :]\n",
    "    \n",
    "    if self.using_prior:\n",
    "      cond_logits = normalize_logits(logits[:-2])\n",
    "      prior_logits = lm_prior(self.curr_string)\n",
    "      post_probs = np.exp(cond_logits + prior_logits)\n",
    "      post_probs = post_probs / np.sum(post_probs) * np.sum(np.exp(logits[:-2]))\n",
    "      logits[:-2] = np.log(post_probs)\n",
    "    \n",
    "    self.action = sample_act(logits)\n",
    "    \n",
    "    self.curr_string += chr(32+onehot_decode(extract_ext_obs(obs)))\n",
    "    \n",
    "  def __call__(self, obs):\n",
    "    return self.action\n",
    "  \n",
    "  def get_hidden_state(self):\n",
    "    assert self.hidden_state.c.shape[0] == 1\n",
    "    return self.hidden_state.c[0, :]\n",
    "  \n",
    "trained_decoder_policy = TrainedDecoderPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_logs = {\n",
    "  'train_loss': [],\n",
    "  'val_loss': [],\n",
    "  'rew': [],\n",
    "  'succ': [],\n",
    "  'ttt': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(batch, step=False, t=None):\n",
    "  batch_obs, batch_act, batch_mask = batch \n",
    "  feed_dict = {\n",
    "    obs_ph: batch_obs,\n",
    "    act_ph: batch_act,\n",
    "    mask_ph: batch_mask,\n",
    "    init_state_a_ph: np.zeros((batch_obs.shape[0], num_hidden)),\n",
    "    init_state_b_ph: np.zeros((batch_obs.shape[0], num_hidden))\n",
    "  }\n",
    "  loss_eval = sess.run(loss, feed_dict=feed_dict)\n",
    "  \n",
    "  if step:\n",
    "    sess.run(update_op, feed_dict=feed_dict)\n",
    "  \n",
    "  d = {'loss': loss_eval}\n",
    "  if not step:\n",
    "    _, val_perf = evaluate_decoder_policy(\n",
    "      trained_decoder_policy, \n",
    "      env=make_env(train_goal=False), \n",
    "      n_rollouts=n_val_eval_rollouts\n",
    "    )\n",
    "    d.update(val_perf)\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_log = None\n",
    "while len(train_logs['train_loss']) < iterations:\n",
    "  batch = sample_batch(batch_size)\n",
    "  \n",
    "  t = len(train_logs['train_loss'])\n",
    "  train_log = compute_batch_loss(batch, step=True, t=t)\n",
    "  if val_log is None or t % val_update_freq == 0:\n",
    "    val_log = compute_batch_loss(val_batch, step=False, t=t)\n",
    "    \n",
    "  if t % agg_freq == 0:\n",
    "    aggregate_rollouts()\n",
    "  \n",
    "  print('%d %d %f %f %f %f %f' % (\n",
    "    t, iterations, train_log['loss'], val_log['loss'], \n",
    "    val_log['rew'], val_log['succ'], val_log['ttt']))\n",
    "  \n",
    "  for k, v in train_log.items():\n",
    "    train_logs['%s%s' % ('train_' if k == 'loss' else '', k)].append(v)\n",
    "  for k, v in val_log.items():\n",
    "    train_logs['%s%s' % ('val_' if k == 'loss' else '', k)].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.plot(train_logs['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Reward')\n",
    "plt.axhline(y=oracle_perf['rew'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['rew'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['rew'], color='orange', label='Trained')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.axhline(y=oracle_perf['succ'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['succ'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['succ'], color='orange', label='Trained')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Time to Target')\n",
    "plt.axhline(y=oracle_perf['ttt'], linestyle='--', color='teal', label='Oracle')\n",
    "plt.axhline(y=rand_perf['ttt'], linestyle=':', color='gray', label='Random')\n",
    "plt.plot(train_logs['ttt'], color='orange', label='Trained')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(train_goal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = run_ep(trained_decoder_policy, env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_succ(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
